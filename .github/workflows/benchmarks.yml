name: Performance benchmarks

on:
  pull_request_target:
    types: [opened, ready_for_review, labeled]
    branches:
      - main
    paths:
      - '**.py'
      - '.github/workflows/benchmarks.yml'

permissions:
  issues: write
  pull-requests: write

jobs:
  run-benchmarks:
    if: >
      github.event.action == 'labeled' && contains(github.event.pull_request.labels.*.name, 'trigger-benchmarks') ||
      github.event.action == 'opened' ||
      github.event.action == 'ready_for_review'
    runs-on: ubuntu-latest
    steps:
      # Python and dependency setup
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'
      - name: Add project directory to PYTHONPATH
        run: echo "PYTHONPATH=$PYTHONPATH:$(pwd)" >> $GITHUB_ENV
      - name: Install uv
        run: pip install uv
      - name: Install dependencies
        run: uv pip install --system numpy pandas tqdm tabulate matplotlib solara networkx scipy
      # Benchmarks on the projectmesa main branch
      - name: Checkout main branch
        uses: actions/checkout@v4
        with:
          ref: main
          repository: projectmesa/mesa
      - name: Install Mesa
        run: pip install --no-deps .
      - name: Run benchmarks on main branch
        working-directory: benchmarks
        run: python global_benchmark.py
      # Upload benchmarks, checkout PR branch, download benchmarks
      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: timings-main
          path: benchmarks/timings_1.pickle
      - name: Checkout PR branch
        uses: actions/checkout@v4
        if: github.event_name == 'pull_request_target'
        with:
          ref: ${{ github.event.pull_request.head.ref }}
          repository: ${{ github.event.pull_request.head.repo.full_name }}
          fetch-depth: 0
          persist-credentials: false
          clean: false
      - name: Install Mesa of the PR branch
        run: pip install --no-deps .
      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: timings-main
          path: benchmarks
      # Run benchmarks on the PR branch
      - name: Run benchmarks on PR branch
        working-directory: benchmarks
        run: python global_benchmark.py
      # Run compare script and create comment
      - name: Run compare timings and encode output
        working-directory: benchmarks
        run: |
          TIMING_COMPARISON=$(python compare_timings.py | base64 -w 0)  # Base64 encode the output
          echo "TIMING_COMPARISON=$TIMING_COMPARISON" >> $GITHUB_ENV
      - name: Comment PR
        uses: actions/github-script@v7
        with:
          script: |
            const output = Buffer.from(process.env.TIMING_COMPARISON, 'base64').toString('utf-8');
            const issue_number = context.issue.number;
            github.rest.issues.createComment({
              issue_number: issue_number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: 'Performance benchmarks:\n\n' + output
            });
